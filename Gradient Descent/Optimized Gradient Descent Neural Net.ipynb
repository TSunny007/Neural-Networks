{
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#adapted from https://iamtrask.github.io/2015/07/27/python-network-part2/\nimport numpy as np # linear algebra",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "alphas = [0.001,0.01,0.1,1,10,100,1000]",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "8ada67fcdf0c5c8960737f0eb1b8368560c4997d",
        "_cell_guid": "0becd16b-9347-43d5-89cb-21d6c08a50df",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def sigmoid(x):\n    return (1/(1+np.exp(-x)))",
      "execution_count": 35,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "e0ab6fea3c2094c0ce38a1c79612060292feb392",
        "_cell_guid": "34742f4c-95bc-4845-95f8-4e76334f65b7",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def sigmoid_derivative(sigmoid_output):\n    return (sigmoid_output*(1-sigmoid_output))",
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "703be98c62683f1204cb0908b35e52b3fe969f13",
        "_cell_guid": "a7117ead-ac10-4dd5-9484-1e0933f55970",
        "trusted": true
      },
      "cell_type": "code",
      "source": "X = np.array([[0,0,1],[0,1,1], [1,0,1], [1,1,1]])",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "616ecf85b6bfa91c0f66dc9dd81393aaae3a916e",
        "_cell_guid": "2e467111-200c-43e9-b853-d7175154dbaf",
        "trusted": true
      },
      "cell_type": "code",
      "source": "y = np.array([[0],[1],[1],[0]])",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "baaef4a2b2152d9655f2ecedcb1f1ba5134030a0",
        "_cell_guid": "dabc42f7-71f1-4d1e-ab0b-913a0c80897c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.random.seed(1)",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cab9c67c49307bf59e260a437780f9228f2aed13",
        "_cell_guid": "554a8f7e-6a91-442d-8934-9f794ddd7b44",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#randomly initialize out weights with mean 0\nsynapse_0 = 2*np.random.random((3,4)) - 1\nsynapse_1 = 2*np.random.random((4,1)) - 1\nprint ('Synapse 0: \\n', synapse_0,'\\nSynapse 1: \\n',synapse_1)",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Synapse 0: \n [[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n [-0.20646505  0.07763347 -0.16161097  0.370439  ]] \nSynapse 1: \n [[-0.5910955 ]\n [ 0.75623487]\n [-0.94522481]\n [ 0.34093502]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "1b32def6cc81f76bfe0d8662fd482ee4e31f3c63"
      },
      "cell_type": "markdown",
      "source": "We will be creating a two layer Neural Network with the first layer having 4 weights and three neurons, and the second layer having one weight and 4 neurons."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9ee751adb070f5bc6a64e4e6e3467d25120cbec0"
      },
      "cell_type": "code",
      "source": "for alpha in alphas:\n    print('\\nTraining With Alpha:',alpha)\n    np.random.seed(1)\n    #randomly initialize our weights with mean 0\n    synapse_0 = 2*np.random.random((3,4)) - 1\n    synapse_1 = 2*np.random.random((4,1)) - 1\n    \n    for step in range(60_000):\n        # Feed forward through layers 0, 1, and 2\n        layer_0 = X\n        layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n\n        # calculate how much we are off by\n        layer_2_error = layer_2 - y\n        # what direction is the desired value?\n        # decide how much to change based on how confident we are \n        layer_2_delta = layer_2_error*sigmoid_derivative(layer_2)\n\n        if step%10000 == 0:\n            print('Error after ',step,' iterations: ', np.mean(np.abs(layer_2_error)))\n        # how much did each layer_1 value contribute to the layer_2 error (according to hte weights)?\n        layer_1_error = layer_2_delta.dot(synapse_1.T)\n        layer_1_delta = layer_1_error*sigmoid_derivative(layer_1)\n        # what direction is the target layer_1? chane based on confidence\n        synapse_1 = synapse_1 - alpha * (layer_1.T.dot(layer_2_delta))\n        synapse_0 = synapse_0 - alpha * (layer_0.T.dot(layer_1_delta))\n    if alpha is 10:\n        print('Synapse 0: \\n', synapse_0,'\\nSynapse 1: \\n',synapse_1)",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": "\nTraining With Alpha: 0.001\nError after  0  iterations:  0.49641003190272537\nError after  10000  iterations:  0.49516402549338606\nError after  20000  iterations:  0.4935960431880486\nError after  30000  iterations:  0.4916063585594306\nError after  40000  iterations:  0.48910016654420474\nError after  50000  iterations:  0.48597785784615843\n\nTraining With Alpha: 0.01\nError after  0  iterations:  0.49641003190272537\nError after  10000  iterations:  0.45743107444190134\nError after  20000  iterations:  0.35909720256339894\nError after  30000  iterations:  0.2393581371589725\nError after  40000  iterations:  0.14307065901337035\nError after  50000  iterations:  0.09859642980892715\n\nTraining With Alpha: 0.1\nError after  0  iterations:  0.49641003190272537\nError after  10000  iterations:  0.0428880170001158\nError after  20000  iterations:  0.024098994228521613\nError after  30000  iterations:  0.018110652146797846\nError after  40000  iterations:  0.01498761627221092\nError after  50000  iterations:  0.013014490538142583\n\nTraining With Alpha: 1\nError after  0  iterations:  0.49641003190272537\nError after  10000  iterations:  0.008584525653247153\nError after  20000  iterations:  0.005789459862507806\nError after  30000  iterations:  0.004629176776769985\nError after  40000  iterations:  0.0039587652802736475\nError after  50000  iterations:  0.003510122567861677\n\nTraining With Alpha: 10\nError after  0  iterations:  0.49641003190272537\nError after  10000  iterations:  0.003129388763011837\nError after  20000  iterations:  0.002144595579852179\nError after  30000  iterations:  0.0017239754995622308\nError after  40000  iterations:  0.0014782145122908034\nError after  50000  iterations:  0.0013127406283356764\nSynapse 0: \n [[ 4.52597806  5.77663165 -7.34266481 -5.29379829]\n [ 1.66715206 -7.16447274 -7.99779235 -1.81881849]\n [-4.27032921 -3.35838279  3.44594007  4.88852208]] \nSynapse 1: \n [[ -8.58485788]\n [ 10.1786297 ]\n [-14.87601886]\n [  7.57026121]]\n\nTraining With Alpha: 100\nError after  0  iterations:  0.49641003190272537\nError after  10000  iterations:  0.12547698383358538\nError after  20000  iterations:  0.12533033354043677\nError after  30000  iterations:  0.12526772879373652\nError after  40000  iterations:  0.12523107370012884\nError after  50000  iterations:  0.12520635280373757\n\nTraining With Alpha: 1000\nError after  0  iterations:  0.49641003190272537\nError after  10000  iterations:  0.5\nError after  20000  iterations:  0.5\nError after  30000  iterations:  0.5\nError after  40000  iterations:  0.5\nError after  50000  iterations:  0.5\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "0cbed8770e038daa27bc8c60d4fcdae1326eca41"
      },
      "cell_type": "markdown",
      "source": "# Analysis\n## Alpha = 0.001 \nThe network with a crazy small alpha didn't hardly converge! This is because we made the weight updates so small that they hardly changed anything, even after 60,000 iterations!\n## Alpha = 0.001 \nThis alpha made a rather pretty convergence. It was quite smooth over the course of the 60,000 iterations but ultimately didn't converge as far as some of the others.\n## Alpha = 0.1\nThis alpha made some of progress very quickly but then slowed down a bit. We need to increase alpha some more.\n## Alpha = 1\nAs a clever eye might suspect, this had the exact convergence as if we had no alpha at all! Multiplying our weight updates by 1 doesn't change anything.\n## Alpha = 10\nAn alpha that was greater than 1 achieved the best score after only 10,000 iterations! This tells us that our weight updates were being too conservative with smaller alphas. This means that in the smaller alpha parameters (less than 10), the network's weights were generally headed in the right direction, they just needed to hurry up and get there!\n## Alpha = 100\nNow we can see that taking steps that are too large can be very counterproductive. The network's steps are so large that it can't find a reasonable lowpoint in the error plane. The Alpha is too big so it just jumps around on the error plane and never \"settles\" into a local minimum.\n## Alpha = 1000\nAnd with an extremely large alpha, we see a textbook example of divergence, with the error increasing instead of decreasing... hardlining at 0.5. This is a more extreme version of overstepping where it overcorrectly whildly and ends up very far away from any local minimums.\n"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}