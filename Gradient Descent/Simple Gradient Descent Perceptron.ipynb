{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# adapted from https://iamtrask.github.io/2015/07/27/python-network-part2/\nimport numpy as np # linear algebra",
      "execution_count": 108,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "32c0f0c0ed403af859a6f8252c0db898c349637e"
      },
      "cell_type": "markdown",
      "source": "# Simple Gradient Descent Perceptron\n![](http://mblogthumb2.phinf.naver.net/MjAxNzAzMjNfMjI4/MDAxNDkwMjI3NzU2MTAw.CnHFk7Sv7sxrQ7LPj3MiIkXfLeVslL04pNQZ0aEAjcMg.54JHq2BlIqLZKjik-Q1MyKheooaPZ9yF-bBVvKSu73Ug.PNG.htk1019/image.png?type=w800)We will be using a very rudimentary version of gradient descent to see a basic classification perceptron modeled as the picture above to classify a *very* simple dataset.\n\n$\\theta$ above is equal to  $\\sum_{i=1}^{2} x_i\\cdot w_i$"
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# compute sigmoid nonlinearity\ndef sigmoid(x):\n    output = 1/(1+np.exp(-x))\n    return output",
      "execution_count": 142,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "aad2b3af-23ff-460d-b31c-7046e241e6ee",
        "_uuid": "3aa4b0ddbb755606b89c7e0d50af6442047829c8",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#using derivative formula from https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/\n# convert OUTPUT of sigmoid function to its derivative\ndef sigmoid_derivative(sigmoid_output):\n    return sigmoid_output * (1-sigmoid_output)",
      "execution_count": 143,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "b1667b15-f83d-4a8d-93d5-eb65015e14ee",
        "_uuid": "f243e872682bf90046e8ce3e9c1afc5163631dac",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# input dataset. Think about a classification perceptron using only 0 and 1 as inputs\nX = np.array([[0,1],[0,1],[1,0],[1,0]])",
      "execution_count": 144,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "1cef978a-f944-46dc-ad64-265a5f0fe419",
        "_uuid": "40162a37defbfa9011996cfd3e05da9b55d260fb",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# output dataset. The bias alludes that having a 1 on the first value gives a 1 output \ny = np.array([[0,0,1,1]]).T",
      "execution_count": 145,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "0be2cf74-2dfc-42d9-a293-3e28321b65e4",
        "_uuid": "7a3de4ec97e833a3ccf220fa8ae77f76a457edca",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# seed random numbers to make calculation\n# deterministic (For consistent results)\nnp.random.seed(1)",
      "execution_count": 146,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ed30eb79-9519-4429-bc74-ad2f656c2d2d",
        "_uuid": "7ba5775a0fe1a5becd6edfb9f90feceb05239cf0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# initialize 2 weights randomly with mean 0\nsynapse_0 = 2*np.random.random((2,1)) - 1\nprint('original synapse weights\\n',synapse_0)",
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": "original synapse weights\n [[-0.16595599]\n [ 0.44064899]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "cdead163-13e9-4e2a-98a3-5b5cb4e63a85",
        "_uuid": "76877196fe04843a3b3eea176a362df3f6a81a8f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "layer_0 = X\n#[[-0.165*(0)+0.44*(1)] = [0.44]. Sigmoid(0.44) = 0.60\n#[-0.165*(0)+0.44*(1)] = [0.44]. Sigmoid(0.44) = 0.60\n#[-0.165*(0)+0.44*(1)] = [-0.165]. Sigmoid(-0.165) = -0.54\n#[-0.165*(0)+0.44*(1)]] = [-0.165]. Sigmoid(-0.165) = -0.54\nlayer_1 = sigmoid(np.dot(layer_0,synapse_0))\nlayer_1_error = layer_1 - y\nprint('our layer 1 predictions\\n',layer_1,'\\n our layer 1 error compared to actual outputs\\n', layer_1_error)",
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": "our layer 1 predictions\n [[0.60841366]\n [0.60841366]\n [0.45860596]\n [0.45860596]] \n our layer 1 error compared to actual outputs\n [[ 0.60841366]\n [ 0.60841366]\n [-0.54139404]\n [-0.54139404]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "be7ff230-9de5-4f16-abdc-7546ae680b71",
        "_uuid": "b83c30eeedeb7fd0d27ecef7b7ab302242f3d664",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Multiply how much we missed by in layer_1  with the slope of the sigmoid values in layer_1. This helps us figure out our \"descent\"\nlayer_1_delta = layer_1_error * sigmoid_derivative(layer_1)\nsynapse_derivative = np.dot(layer_0.T, layer_1_delta)\n\nprint('how much we should adjust our synapse values by\\n',synapse_derivative)\nsynapse_0 -= synapse_derivative\nprint('new synapse value\\n',synapse_0)",
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": "how much we should adjust our synapse values by\n [[-0.2688417 ]\n [ 0.28990482]]\nnew synapse value\n [[0.10288571]\n [0.15074416]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5634c8db643a3458cb9b034fe44acc5bd59269af"
      },
      "cell_type": "code",
      "source": "for iter in range(10000):\n\n    # forward propagation\n    layer_0 = X\n    layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n\n    # how much did we miss?\n    layer_1_error = layer_1 - y\n    # multiply how much we missed by the \n    # slope of the sigmoid at the values in l1\n    layer_1_delta = layer_1_error * sigmoid_derivative(layer_1)\n    synapse_0_derivative = np.dot(layer_0.T,layer_1_delta)\n\n    # update weights\n    synapse_0 -= synapse_0_derivative\n\nprint (\"Output After Training:\")\n# Remember these values are called by the sigmoid function. \n# The asymtote at y=1 for the sigmoid function is why our synapse weights keep growing but the prediction never quite gets perfect\nprint ('Synapse weights\\n',synapse_0)\nprint ('Synapse predictions\\n',layer_1)",
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Output After Training:\nSynapse weights\n [[ 5.28321694]\n [-5.28316892]]\nSynapse predictions\n [[0.00505094]\n [0.00505094]\n [0.99494931]\n [0.99494931]]\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}